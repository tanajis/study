Note : Always download binary. Binary means you can diretly use, no need to compile.
If you download source, it is java code only, ypu need to comile ,create binary and later need to use that binary.



#-------------------------------------------------------------------------------------
Windows setup for spark:
#-------------------------------------------------------------------------------------
Ref 
https://www.youtube.com/watch?v=e6j5IHOERIM&t=53s
https://www.youtube.com/watch?v=WQErwxRTiW0



 1.download webutil.exe and put in  C:\winutil\bin
 https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin
 
 set   below system environment variables
 HADOOP_HOME   C:\winutil
 SPARK_HOME C:\spark-2.4.3-bin-hadoop2.7
 PYSPARK_PYTHON  C:\Python27\python.exe
 
 2.set java path
 3.install scala
 install python https://www.python.org/downloads/release/python-2716/
 4.add environment variable 



#-------------------

SPARK_MASTER_PORT	Start the master on a different port (default: 7077).
SPARK_MASTER_WEBUI_PORT	Port for the master web UI (default: 8080).
SPARK_WORKER_WEBUI_PORT	Port for the worker web UI (default: 8081).

add to PATH  
%SPARK_HOME%\bin   
C:\Python27\python.exe

Note : if not working in visual studi or pycharm close & restart visual studio


#-------------------------------------------------------------------------------------
Spark on ubuntu(Linux)
#-------------------------------------------------------------------------------------
1.install scala : sudp apt-get install scala

2.



Note java version must be matchung with java version specified on spark's website.
It was java 8 and not java 8+ SO java 11 was not working for me. and giving error :
pyspark.sql.utils.IllegalArgumentException: 'Unsupported class file major version 55' [duplicate]

export HADOOP_HOME=/home/tms/myInstallDir/hadoop-2.8.0
export PATH=$PATH:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export SPARK_HOME=/home/tms/myInstallDir/spark-2.4.5-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
export PATH=$PATH:$SPARK_HOME/python
export PYSPARK_PYTHON=/usr/bin/python

#-----------------------------------
STANDALONE MODE of spark 
#-----------------------------------
Note : JAVA_HOME must be set before moving for below.
-This used standalone mode.
-Sparks own cluster manager is used.
-local file system is used.
-No hadoop,yarn is used in this mode.

step 1: just extract folder 
step 2: go to sbin folder of spark and fire :  ./start-all.sh
step 3:go to bin folder and fire : pyspark or spark-shell

#-----------------------------------
SPEDO DISTRIBUTED MODE of Spark
#-----------------------------------



#----------------------------------------------------------------------------------------------------
Spark with User provided hadoop (Ubuntu and linuxmint - hadoop/yarn mode)
#---------------------------------------------------------------------------------------------------
-This option means that you must provide your hadoop jars, assuming it is installed on your cluster
-Any hadoop version. Hadoop needs to be installed and HADOOP_HOME must be set already)

1.download spark without hadoop(user provided hadoop) and extract  Ex. spark-2.4.5-bin-without-hadoop.tgz

2.go to spark/conf directory and copy slaves.template to slaves
  It contains hostname or IP of slaves. By default it has only one that is localhost

3.copy spark-env.sh.template to spark-env.sh and add below at the end:

export SPARK_DIST_CLASSPATH=$(/home/tms/myInstallDir/hadoop-2.10.0/bin/hadoop classpath)
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export SPARK_MASTER_HOST=localhost

 

Spark Defaults has below :
# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
 
Note : 
https://spark.apache.org/docs/2.3.1/hadoop-provided.html
https://stackoverflow.com/questions/35502046/what-is-pre-build-with-user-provided-hadoop-package

### in conf/spark-env.sh ###
Any one of below:

# If 'hadoop' binary is on your PATH
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

or 

# With explicit path to 'hadoop' binary
export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)

or 
# Passing a Hadoop configuration directory
export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)

Ref : https://medium.com/ymedialabs-innovation/apache-spark-on-a-multi-node-cluster-b75967c8cb2b
