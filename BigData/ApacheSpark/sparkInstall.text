
#-------------------------------------------------------------------------------------
Windows setup for spark:
#-------------------------------------------------------------------------------------
Ref 
https://www.youtube.com/watch?v=e6j5IHOERIM&t=53s
https://www.youtube.com/watch?v=WQErwxRTiW0



 1.download webutil.exe and put in  C:\winutil\bin
 https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin
 
 set   below system environment variables
 HADOOP_HOME   C:\winutil
 SPARK_HOME C:\spark-2.4.3-bin-hadoop2.7
 PYSPARK_PYTHON  C:\Python27\python.exe
 
 2.set java path
 3.install scala
 install python https://www.python.org/downloads/release/python-2716/
 4.add environment variable 


add to PATH  
%SPARK_HOME%\bin   
C:\Python27\python.exe

Note : if not working in visual studi or pycharm close & restart visual studio


#-------------------------------------------------------------------------------------
Spark on ubuntu(Linux)
#-------------------------------------------------------------------------------------

Note java version must be matchung with java version specified on spark's website.
It was java 8 and not java 8+ SO java 11 was not working for me. and giving error :
pyspark.sql.utils.IllegalArgumentException: 'Unsupported class file major version 55' [duplicate]

export HADOOP_HOME=/home/tms/myInstallDir/hadoop-2.8.0
export PATH=$PATH:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export SPARK_HOME=/home/tms/myInstallDir/spark-2.4.5-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
export PATH=$PATH:$SPARK_HOME/python
export PYSPARK_PYTHON=/usr/bin/python

#-----------------------------------
STANDALONE MODE of spark 
#-----------------------------------
Note : JAVA_HOME must be set before moving for below.
-This used standalone mode.
-Sparks own cluster manager is used.
-local file system is used.
-No hadoop,yarn is used in this mode.

step 1: just extract folder 
step 2: go to sbin folder of spark and fire :  ./start-all.sh
step 3:go to bin folder and fire : pyspark or spark-shell

#-----------------------------------
SPEDO DISTRIBUTED MODE of Spark
#-----------------------------------



#-------------------------------------
Spark with User provided hadoop
#------------------------------------
This option means that you must provide your hadoop jars, assuming it is installed on your cluster

https://spark.apache.org/docs/2.3.1/hadoop-provided.html
https://stackoverflow.com/questions/35502046/what-is-pre-build-with-user-provided-hadoop-package



### in conf/spark-env.sh ###

# If 'hadoop' binary is on your PATH
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# With explicit path to 'hadoop' binary
export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)

# Passing a Hadoop configuration directory
export SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)


